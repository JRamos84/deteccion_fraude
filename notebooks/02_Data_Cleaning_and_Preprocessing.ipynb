{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Data Cleaning and Preprocessing\n",
    "**Objetivo**: Limpiar y preprocesar los datos para que estén listos para el modelado.\n",
    "**Contenido**:\n",
    "- Manejo de valores nulos.\n",
    "- Creación de nuevas características (feature engineering).\n",
    "- Conversión de tipos de datos.\n",
    "- Codificación de variables categóricas.\n",
    "- Normalización y estandarización de las variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Librerías \n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cargamos los Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trabajaremos exclusivamente con el dataset de entrenamiento \"train_transaction\" para obtener tanto los datos de entrenamiento como los de validación. De este dataset solo tomaremos el 10 % del total de los registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_col_transaction = 'TransactionID,isFraud,TransactionDT,TransactionAmt,ProductCD,card1,card2,card3,card4,card5,card6,addr1,addr2,dist1,dist2,P_emaildomain,R_emaildomain,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,D1,D2,D3,D4,D5,D6,D7,D8,D9,D10,D11,D12,D13,D14,D15,M1,M2,M3,M4,M5,M6,M7,M8,M9'.split(',')\n",
    "select_col_identity = 'TransactionID,id_01,id_02,id_03,id_04,id_05,id_06,id_07,id_08,id_09,id_10,id_11,id_12,id_13,id_14,id_15,id_16,id_17,id_18,id_19,id_20,id_21,id_22,id_23,id_24,id_25,id_26,id_27,id_28,id_29,id_30,id_31,id_32,id_33,id_34,id_35,id_36,id_37,id_38,DeviceType,DeviceInfo'.split(',')\n",
    "data_dir = '../data/raw/ieee-fraud-detection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de porción del Dataset de entrenamiento con Muestreo Estratificado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 59054 entries, 294161 to 404259\n",
      "Data columns (total 95 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   TransactionID   59054 non-null  int64  \n",
      " 1   isFraud         59054 non-null  int64  \n",
      " 2   TransactionDT   59054 non-null  int64  \n",
      " 3   TransactionAmt  59054 non-null  float64\n",
      " 4   ProductCD       59054 non-null  object \n",
      " 5   card1           59054 non-null  int64  \n",
      " 6   card2           58127 non-null  float64\n",
      " 7   card3           58896 non-null  float64\n",
      " 8   card4           58896 non-null  object \n",
      " 9   card5           58658 non-null  float64\n",
      " 10  card6           58896 non-null  object \n",
      " 11  addr1           52441 non-null  float64\n",
      " 12  addr2           52441 non-null  float64\n",
      " 13  dist1           23737 non-null  float64\n",
      " 14  dist2           3818 non-null   float64\n",
      " 15  P_emaildomain   49586 non-null  object \n",
      " 16  R_emaildomain   13850 non-null  object \n",
      " 17  C1              59054 non-null  float64\n",
      " 18  C2              59054 non-null  float64\n",
      " 19  C3              59054 non-null  float64\n",
      " 20  C4              59054 non-null  float64\n",
      " 21  C5              59054 non-null  float64\n",
      " 22  C6              59054 non-null  float64\n",
      " 23  C7              59054 non-null  float64\n",
      " 24  C8              59054 non-null  float64\n",
      " 25  C9              59054 non-null  float64\n",
      " 26  C10             59054 non-null  float64\n",
      " 27  C11             59054 non-null  float64\n",
      " 28  C12             59054 non-null  float64\n",
      " 29  C13             59054 non-null  float64\n",
      " 30  C14             59054 non-null  float64\n",
      " 31  D1              58914 non-null  float64\n",
      " 32  D2              30944 non-null  float64\n",
      " 33  D3              32667 non-null  float64\n",
      " 34  D4              42112 non-null  float64\n",
      " 35  D5              28016 non-null  float64\n",
      " 36  D6              7371 non-null   float64\n",
      " 37  D7              3899 non-null   float64\n",
      " 38  D8              7610 non-null   float64\n",
      " 39  D9              7610 non-null   float64\n",
      " 40  D10             51378 non-null  float64\n",
      " 41  D11             31028 non-null  float64\n",
      " 42  D12             6520 non-null   float64\n",
      " 43  D13             6205 non-null   float64\n",
      " 44  D14             6210 non-null   float64\n",
      " 45  D15             50135 non-null  float64\n",
      " 46  M1              31817 non-null  object \n",
      " 47  M2              31817 non-null  object \n",
      " 48  M3              31817 non-null  object \n",
      " 49  M4              31046 non-null  object \n",
      " 50  M5              24098 non-null  object \n",
      " 51  M6              42075 non-null  object \n",
      " 52  M7              24254 non-null  object \n",
      " 53  M8              24256 non-null  object \n",
      " 54  M9              24256 non-null  object \n",
      " 55  id_01           14515 non-null  float64\n",
      " 56  id_02           14179 non-null  float64\n",
      " 57  id_03           6727 non-null   float64\n",
      " 58  id_04           6727 non-null   float64\n",
      " 59  id_05           13777 non-null  float64\n",
      " 60  id_06           13777 non-null  float64\n",
      " 61  id_07           521 non-null    float64\n",
      " 62  id_08           521 non-null    float64\n",
      " 63  id_09           7610 non-null   float64\n",
      " 64  id_10           7610 non-null   float64\n",
      " 65  id_11           14187 non-null  float64\n",
      " 66  id_12           14515 non-null  object \n",
      " 67  id_13           12806 non-null  float64\n",
      " 68  id_14           8048 non-null   float64\n",
      " 69  id_15           14187 non-null  object \n",
      " 70  id_16           12999 non-null  object \n",
      " 71  id_17           14025 non-null  float64\n",
      " 72  id_18           4540 non-null   float64\n",
      " 73  id_19           14025 non-null  float64\n",
      " 74  id_20           14018 non-null  float64\n",
      " 75  id_21           522 non-null    float64\n",
      " 76  id_22           523 non-null    float64\n",
      " 77  id_23           523 non-null    object \n",
      " 78  id_24           490 non-null    float64\n",
      " 79  id_25           519 non-null    float64\n",
      " 80  id_26           521 non-null    float64\n",
      " 81  id_27           523 non-null    object \n",
      " 82  id_28           14187 non-null  object \n",
      " 83  id_29           14187 non-null  object \n",
      " 84  id_30           7793 non-null   object \n",
      " 85  id_31           14110 non-null  object \n",
      " 86  id_32           7796 non-null   float64\n",
      " 87  id_33           7374 non-null   object \n",
      " 88  id_34           7817 non-null   object \n",
      " 89  id_35           14187 non-null  object \n",
      " 90  id_36           14187 non-null  object \n",
      " 91  id_37           14187 non-null  object \n",
      " 92  id_38           14187 non-null  object \n",
      " 93  DeviceType      14172 non-null  object \n",
      " 94  DeviceInfo      11958 non-null  object \n",
      "dtypes: float64(60), int64(4), object(31)\n",
      "memory usage: 43.3+ MB\n"
     ]
    }
   ],
   "source": [
    "## dataset de entrenamiento\n",
    "seed = 42\n",
    "# Cargar los datos\n",
    "df_transaction_train = pd.read_csv(data_dir + '/train_transaction.csv', usecols=select_col_transaction)\n",
    "df_identity_train = pd.read_csv(data_dir + '/train_identity.csv', usecols=select_col_identity)\n",
    "\n",
    "# Combinar los datasets\n",
    "dataset = pd.merge(df_transaction_train, df_identity_train, on='TransactionID', how='left')\n",
    "\n",
    "# Realizar el muestreo estratificado\n",
    "data, _ = train_test_split(dataset, stratify=dataset['isFraud'], test_size=0.9, random_state=seed)\n",
    "\n",
    "# Mostrar la información del dataset resultante\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armamos un nuevo dataset con las columnas más relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['isFraud', 'TransactionDT', 'TransactionAmt',\n",
    "       'ProductCD','addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n",
    "       'DeviceType', 'DeviceInfo']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basándonos en el análisis exploratorio de datos (EDA) realizado previamente, eliminamos las columnas que contienen más del 80% de valores nulos.\n",
    "- Eliminamos las filas con valores nulos debido a que tenemos suficientes cantidad de registros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['dist1','DeviceType','R_emaildomain','DeviceInfo','dist2'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isFraud           0\n",
       "TransactionDT     0\n",
       "TransactionAmt    0\n",
       "ProductCD         0\n",
       "addr1             0\n",
       "addr2             0\n",
       "P_emaildomain     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversión de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es necesatio convertir los tipos de datos, debido a que son adecuado al tipo de datos que ya tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isFraud             int64\n",
       "TransactionDT       int64\n",
       "TransactionAmt    float64\n",
       "ProductCD          object\n",
       "addr1             float64\n",
       "addr2             float64\n",
       "P_emaildomain      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separamos datos características (X) y variable objetivo (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('isFraud', axis=1)  # features\n",
    "y = data['isFraud']  # target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de nuevas características (feature engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionAmt_Range\n",
      "Medio       22127\n",
      "Bajo         9636\n",
      "Alto         6940\n",
      "Muy alto     4498\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calcular los cuartiles y el IQR\n",
    "Q1 = np.percentile(X['TransactionAmt'], 25)\n",
    "Q3 = np.percentile(X['TransactionAmt'], 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Definir los umbrales para valores atípicos\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Función para clasificar los valores en rangos\n",
    "def classify_transaction_amt(value):\n",
    "    if value < lower_bound:\n",
    "        return 'Muy bajo'\n",
    "    elif lower_bound <= value < Q1:\n",
    "        return 'Bajo'\n",
    "    elif Q1 <= value < Q3:\n",
    "        return 'Medio'\n",
    "    elif Q3 <= value < upper_bound:\n",
    "        return 'Alto'\n",
    "    else:\n",
    "        return 'Muy alto'\n",
    "\n",
    "# Aplicar la función de clasificación a la columna\n",
    "X['TransactionAmt_Range'] = X['TransactionAmt'].apply(classify_transaction_amt)\n",
    "\n",
    "\n",
    "# Mostrar algunos resultados\n",
    "print(X['TransactionAmt_Range'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminamos la columna 'TransactionAmt' por la conclusiones obtenidas en EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop('TransactionAmt', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntar addr1 y addr2\n",
    "\n",
    "- addr1 representa una región dentro de un país, mientras que addr2 corresponde al código de país. Combinar estas variables podría capturar de manera más efectiva la relación geográfica entre regiones y países en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['addr1'] = X['addr1'].astype('int').astype('str')\n",
    "X['addr2'] = X['addr2'].astype('int').astype('str')\n",
    "\n",
    "# Concatenar addr1 y addr2 en una nueva columna addr_combined usando +\n",
    "X['addr_combined'] = X['addr1'] + '_' + X['addr2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionDT', 'ProductCD', 'addr1', 'addr2', 'P_emaildomain',\n",
       "       'TransactionAmt_Range', 'addr_combined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliminamos addr1 y addr2 de X\n",
    "X.drop(['addr1', 'addr2'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionDT            int64\n",
       "ProductCD               object\n",
       "P_emaildomain           object\n",
       "TransactionAmt_Range    object\n",
       "addr_combined           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar tipos de datos\n",
    "data_types = X.dtypes\n",
    "\n",
    "# Filtrar y contar variables numéricas y categóricas\n",
    "num_vars = data_types[data_types != 'object']  # Variables numéricas\n",
    "cat_vars = data_types[data_types == 'object']  # Variables categóricas\n",
    "num_vars_names = num_vars.index.tolist()\n",
    "cat_vars_names = cat_vars.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna: ProductCD, el número de variables es: 4\n",
      "Columna: P_emaildomain, el número de variables es: 56\n",
      "Columna: TransactionAmt_Range, el número de variables es: 4\n",
      "Columna: addr_combined, el número de variables es: 160\n"
     ]
    }
   ],
   "source": [
    "for columna in cat_vars_names :\n",
    "    print(f\"Columna: {columna}, el número de variables es: {X[columna].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que cada columna tiene diferentes tipos de categorías, aplicaremos según sea adecuada a cada cado.\n",
    "- ProductCD (4 categorías):\n",
    "    - One-Hot Encoding: Como ProductCD tiene un número pequeño y fijo de categorías (4 en total), el one-hot encoding es una opción adecuada. \n",
    "\n",
    "- TransactionAmt_Range (4 categorías):\n",
    "    - Dado que TransactionAmt_Range tiene un orden natural o ordinal con 4 categorías, el ordinal encoding es apropiado. Asigna valores numéricos secuenciales a cada categoría.\n",
    "- P_emaildomain (56 categorías) y  addr_combined:\n",
    "    - Hashing Trick o Binary Encoding:  Dado que P_emaildomain tiene muchas categorías (56), el one-hot encoding puede generar demasiadas columnas y aumentar la complejidad. binary encoding podrían ser más eficientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294161</th>\n",
       "      <td>7257076.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>Bajo</td>\n",
       "      <td>472_87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34825</th>\n",
       "      <td>855320.0</td>\n",
       "      <td>aol.com</td>\n",
       "      <td>Medio</td>\n",
       "      <td>315_87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374859</th>\n",
       "      <td>9335149.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>Bajo</td>\n",
       "      <td>177_87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545712</th>\n",
       "      <td>14406961.0</td>\n",
       "      <td>comcast.net</td>\n",
       "      <td>Bajo</td>\n",
       "      <td>299_87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518262</th>\n",
       "      <td>13574107.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>Medio</td>\n",
       "      <td>315_87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1      2       3    4    5    6    7\n",
       "294161   7257076.0    gmail.com   Bajo  472_87  NaN  NaN  NaN  NaN\n",
       "34825     855320.0      aol.com  Medio  315_87    0    0  1.0    0\n",
       "374859   9335149.0    gmail.com   Bajo  177_87  NaN  NaN  NaN  NaN\n",
       "545712  14406961.0  comcast.net   Bajo  299_87  NaN  NaN  NaN  NaN\n",
       "518262  13574107.0    gmail.com  Medio  315_87  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43201, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Codificación One-Hot para ProductCD\n",
    "encoder_productcd = OneHotEncoder()\n",
    "X_productcd_encoded = encoder_productcd.fit_transform(X[['ProductCD']])\n",
    "\n",
    "\n",
    "# Convertir la matriz dispersa a DataFrame y verificar su forma\n",
    "X_productcd_encoded_df = pd.DataFrame(X_productcd_encoded.toarray(), \n",
    "                                      columns=encoder_productcd.get_feature_names_out(['ProductCD']),\n",
    "                                      index=X.index)\n",
    "\n",
    "X.drop(['ProductCD'], axis=1,inplace=True)\n",
    "X  = pd.concat([X, X_productcd_encoded_df], axis=1)\n",
    "\n",
    "# Codificación Ordinal para TransactionAmt_Range\n",
    "encoder_transactionamt_range = OrdinalEncoder()\n",
    "X['TransactionAmt_Range'] = encoder_transactionamt_range.fit_transform(X[['TransactionAmt_Range']])\n",
    "\n",
    "# Codificación BinaryEncoder para 'P_emaildomain'\n",
    "encoder_pemaildomain = ce.BinaryEncoder(cols=['P_emaildomain'])\n",
    "X_encoded_pemaildomain = encoder_pemaildomain.fit_transform(X['P_emaildomain'])\n",
    "\n",
    "# Eliminar la columna original 'P_emaildomain' y concatenar las columnas codificadas\n",
    "X = pd.concat([X.drop(['P_emaildomain'], axis=1), X_encoded_pemaildomain], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Codificación BinaryEncoder para 'addr_combined'\n",
    "encoder_addr_combined = ce.BinaryEncoder(cols=['addr_combined'])\n",
    "X_encoded_addr_combined = encoder_addr_combined.fit_transform(X['addr_combined'])\n",
    "\n",
    "# Eliminar la columna original 'addr_combined' y concatenar las columnas codificadas\n",
    "X = pd.concat([X.drop(['addr_combined'], axis=1), X_encoded_addr_combined], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print( X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionDT', 'TransactionAmt_Range', 'ProductCD_C', 'ProductCD_H',\n",
       "       'ProductCD_R', 'ProductCD_W', 'P_emaildomain_0', 'P_emaildomain_1',\n",
       "       'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4',\n",
       "       'P_emaildomain_5', 'addr_combined_0', 'addr_combined_1',\n",
       "       'addr_combined_2', 'addr_combined_3', 'addr_combined_4',\n",
       "       'addr_combined_5', 'addr_combined_6', 'addr_combined_7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización y estandarización de las variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basados en el anális EDA la columna 'TransactionDT' tiene una distribución casi uniforme por lo tanto la normalización Min-Max ajusta los valores de una característica a un rango específico, típicamente entre 0 y 1. Es útil cuando deseas mantener la distribución de los datos pero ajustarla a una escala uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ajustar y transformar los datos de TransactionDT\n",
    "X['TransactionDT_scaled'] = scaler.fit_transform(X[['TransactionDT']])\n",
    "X.drop('TransactionDT',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "En este proyecto de Data Science, el proceso de limpieza y preprocesamiento de datos ha sido fundamental para preparar nuestros datos antes de aplicar modelos de aprendizaje automático. A continuación, se detallan las principales etapas y decisiones tomadas durante este proceso:\n",
    "\n",
    "1. **Selección Estratégica del Dataset**: Se cargó el dataset completo y se seleccionó estratégicamente una porción del 10%, utilizando muestreo estratificado para asegurar representatividad y evitar sesgos en nuestros modelos.\n",
    "\n",
    "2. **Selección de Características Relevantes**: Se llevó a cabo la selección de las columnas más relevantes para nuestro análisis y modelos, descartando aquellas que no contribuían significativamente a la predicción del target.\n",
    "\n",
    "3. **Manejo de Valores Nulos**: Se eliminaron aquellas columnas que contenían un alto porcentaje (mayor al 80%) de valores nulos, así como las filas con un porcentaje bajo de valores nulos para garantizar la integridad de los datos restantes.\n",
    "\n",
    "4. **Separación de Variables**: Se realizó una clara separación entre las variables características (`X`) y el target (`y`), asegurando que estuvieran correctamente definidas para el entrenamiento de los modelos.\n",
    "\n",
    "5. **Ingeniería de Características**: Basados en un análisis exploratorio de datos (EDA), se reemplazó la columna `TransactionAmt` por `TransactionAmt_Range`, una variable categórica que agrupa los valores en rangos como \"muy bajo\", \"bajo\", \"medio\", \"alto\" y \"muy alto\". Esta transformación facilita el manejo de variables con amplios rangos de valores, evitando posibles complicaciones durante el entrenamiento de los modelos.\n",
    "\n",
    "6. **Combinación de Variables**: Se combinaron las columnas `addr1` y `addr2` en una sola columna (`addr_combined`), reduciendo así la dimensionalidad del dataset sin perder información relevante.\n",
    "\n",
    "7. **Codificación de Variables Categóricas**: Se aplicaron técnicas adecuadas de codificación a las variables categóricas según su naturaleza, como codificación one-hot y binary encoding, para prepararlas para su uso en los modelos de aprendizaje automático.\n",
    "\n",
    "8. **Normalización de Variables Numéricas**: Se normalizaron las variables numéricas para asegurar que todas estuvieran en la misma escala, lo cual es crucial para modelos que se basan en la distancia o magnitud de los atributos.\n",
    "\n",
    "En resumen, el proceso de limpieza y preprocesamiento de datos realizado ha permitido transformar el dataset inicial en un formato apto y optimizado para la construcción de modelos predictivos. Estas etapas son fundamentales para asegurar la calidad de los resultados obtenidos y facilitar la interpretación y aplicación de los modelos en la práctica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar X y y en archivos CSV\n",
    "\n",
    "dir_data_processed = '../data/processed'\n",
    "ruta_archivo_X = os.path.join(dir_data_processed, 'datos_procesados.csv')\n",
    "X.to_csv(ruta_archivo_X, index=False, encoding='utf-8')\n",
    "\n",
    "ruta_archivo_y = os.path.join(dir_data_processed, 'target.csv')\n",
    "y.to_csv(ruta_archivo_y, index=False, encoding='utf-8')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
